---
date-start: 2024-10-11 21:00:00
end_date: 2024-10-15
duration: P4D
time: 4 pm CDT
systems: Ceres
locations: All
reason: Ceres Storage Updates
cal-text: "Ceres will be unavailable for maintenance starting at 4pm CDT on Friday October 11th. During this maintenance a new storage system will be put in place. <b>Jobs held during maintenance will require user intervention to run after the maintenance completes.</b> <a href='https://scinet.usda.gov/news/downtime/#ceres-storage-updates-%C2%B7ceres-all%C2%B7-2024'>Read more about these updates on the SCINet Website.</a>"
---

Ceres will be unavailable for maintenance starting at 4pm CDT on Friday October 11th. The final sync for the cutover to the new all-flash Vast storage appliance will start then. Below you will find information on the new storage implementation, information on the transition so far, and actions to take if you are a user who has queued jobs or would like to run jobs with the new storage.

#### Highlights:
Maintenance to cutover to the new storage starts at 4pm CDT 10/11/24 and is planned to run through 10/15/24.
- Users with jobs that will be held over maintenance will be required to issue `scontrol release <JobID>` commands for them to start.
- Retired storage will be available in a read-only state for a limited time.
    - /90daydata-old will be available, read-only, for 90 days while the data ages out.
    - /project-old will be available, read-only, until the final sync to the new /project is done.
- New /project will be available when the final sync finishes if the sync takes longer than the maintenance window.
- New /90daydata will be immediately available (and empty).

#### What is happening:
Ceres is transitioning to a new storage appliance. After the maintenance, /project, /90daydata, and /home directories will all be served from the new Vast appliance. It has several performance and resilience advantages over the retiring storage. Most notable for users is the transition to all flash storage instead of the traditional spinning disks used by the retiring storage.

The reason a maintenance has been dedicated to this cutover is to ensure the smooth and complete transfer of data from the old /project to the new one. Since /project is over 2PB of data spread across more than 1 billion individual files, the transfer takes a considerable amount of time. VRSC has been copying data from /project to the new storage for the last few weeks in preparation for this. We’ve been limiting the transfer to about 100TB/day in order to not impact running jobs. Since there have been files added, removed, and overwritten in the normal day-to-day operations of the cluster while this initial sync has been taking place, a final, complete sync must be done to capture the complete state of the retiring filesystem.

#### What you may have to do:
1. After the maintenance check if you have jobs waiting in the queue with `squeue --me` command.
2. Review your jobs waiting in the queue to identify what storage they need. Run the `scontrol show job <JobID>` command to view information on a submitted job.
3. Release your jobs **AFTER** reading the following

Jobs will be placed into a held state over the maintenance. This is being done to prevent them from running automatically, since the storage may not be in the state the jobs are relying on to run. If the final sync for /project takes longer than the maintenance window, we’re going to make the cluster available without it so jobs can be run in /90daydata. The retiring filesystems will still be available at /90daydata-old (for 90 days while it phases itself out) and /project-old (until the sync to the new /project completes). If you have jobs that are in a held state and you have confirmed that they will be able to access the directories and data they need, you can put them back into the queue with a `scontrol resume <JobID>` command.

**If you have jobs that will require data in its original location in /project and the sync hasn’t finished yet:**  
You can either wait for the sync to finish before running `scontrol resume <JobID>`, or you can cancel the jobs with `scancel <JobID>`, copy the data to /90daydata, and start new jobs working out of /90daydata. This is the most likely scenario for most jobs.

**If you have data in /90daydata-old that you would like to use:**  
Transfer data from /90daydata-old to the new /90daydata to keep it for another 90 days.

**If the new /project isn’t available yet and you would like to run jobs with data from /project-old:**  
Transfer data from /project-old to /90daydata and run your jobs from /90daydata. Directly referencing /project-old in jobs is **NOT** recommended as that storage mount will be removed when the sync is finished.

If you have any questions or concerns, or if you need help after the maintenance, please feel free to contact us at [scinet_vrsc@usda.gov](mailto:scinet_vrsc@usda.gov).